\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2021

% ready for submission
\usepackage[preprint]{neurips_2021}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{neurips_2021}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[colorlinks=true]{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{siunitx}
\usepackage{graphicx}

\newcommand{\todo}[1]{{\color{red} #1}}

\title{Spotify feat. Logistic Regression - \\ Popularity, Nothing Else Matters}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Sebastian Hoffmann\\
  Matrikelnummer 5954377\\
  \texttt{sebastian.hoffmann@student.uni-tuebingen.de} \\
  \And
  Yannick Streicher\\
  Matrikelnummer 5331817\\
  \texttt{yannick.streicher@student.uni-tuebingen.de} \\
}

\begin{document}

\maketitle

\begin{abstract}
  What is the musical taste of the world? With the recent rise and global pervasiveness of music streaming services, such as Spotify, Deezer, or Apple Music, answering this question has become tractable. For this, we plan to analyze a \href{https://www.kaggle.com/rodolfofigueroa/spotify-12m-songs}{subset of 1.2 million songs} scraped from Spotify. However, this dataset lacks crucial information about popularity. Thus, an important step of our work is to augment the dataset further by querying the official Spotify REST API for a randomly sampled subset of the data. Besides a birds-eye overview of the musical landscape, e.g. distribution of genres, we want to identify common musical properties shared by popular songs, and likewise, very unpopular songs, using logistic regression. Such properties can be, for instance, tempo, mode, or key.
\end{abstract}

\section{Introduction}
The questions we want to answer during our project are: ""

\section{Methods and Data Collection}
To pursue our overall goal of predicting the popularity of a song given its Spotify features, we have to find good labels that discriminate popular and unpopular songs. However, the Kaggle dataset\footnote{include link} only contains musical song features. Spotify\footnote{link company} includes some metrics that summarize a measure of popularity, derived by preferences of Spotify's users. In the end, two metrics are important for our project, \textit{(i): Popularity:} An integer between 0 and 100 describing how popular a track is. According to Spotify's API description, this metric is determined algorithmically based on the total number of plays a track has, as well as, how recent those plays are. Artist popularity is derived from the popularity of its individual tracks. The second feature is \textit{(ii): Followers} which is the number of accounts that subscribed to this artists Spotify feed. The number can be interpreted as peoples interest in following the new releases of a particular artist.

The starting point for the project was the 1.2m song dataset on Kaggle. For all of those songs, we queried the Spotify API for additional artist information. In total, we queried around \num[]{85 000} artists, each having a popularity score and a number of followers. Additionally, each artist is described by a set of genres.

\paragraph{Filtering and Preprocessing}
With the advent of online self-publishing, it has became very easy and accessable to distribute home-produced music to a worldwide audience. In fact, there exist specialized companies, such as TuneCore or DistroKid for instance, that automatically distribute submitted songs to multiple platforms at the same time, including Spotify and Apple Music. Indeed we find that a quarter of artists on Spotify have less than $45$ followers and half of all artist have less than $393$. For comparison, \emph{Merikan}, a relatively unknown \emph{Drum and Bass} producer, which is not a major genre, has $4581$ follower at the time of writing. Given that self-published songs do not undergo the feedback and quality control process a regular music label would provide to an artist, we are worried that these tracks can potentially introduce a lot of noise to our data. Consequently, we filter our dataset to only include songs of artists that are to some extent known and established and can be considered professional or semi-professional musicians.

There are two metrics that could potentially be considered as a filter criterion: The popularity of an artists and the number of followers. We find that the number of followers is roughly exponentially distributed. After transforming it logarithmically, there is a clear linear relationship between the number of (log) followers and popularity visible (Pearson correlation coefficient: $0.88$, see Figure~\ref{fig:filtering}~left). This indicates that a more robust measure can be derived by combining both metrics into one.

To this end, we transform both followers and popularity using PCA. The first principal component can be seen as a robust measure of artist popularity, whereas the second component indicates the deviation from the linear relationship. Upon further inspection, we find artists such as \emph{\todo{find the minecraft guy}} that while having a significant amount of followers, do not have any successful songs. To further reduce noise, we filter these outliers as well by thresholding the second principal component. This affects $1366$ artists ($1.6\%$). We then threshold the first component to filter $50\%$ of all artists. Finally, we filter all are artists that do not have a genre associated with them (affects $52.26\%$\footnote{Notice that there is a big overlap with unknown artists, presumably since scarce data makes it difficult for Spotify to establish a genre. \textit{Y: And, maybe they don't have a label / producer that correctly puts the genre labels into the Spotify Database.}}).

In total, if all three steps are applied, $62.8\%$ of all artists are removed from the dataset and $31694$ artists in total are retained. This reduces the number of songs left in the dataset to $755472$, or $62.75\%$ of the original size.

\begin{figure}
  \centering
  \includegraphics[width=0.42\textwidth]{../figures/artists_unfiltered.pdf}
  \qquad
  \includegraphics[width=0.42\textwidth]{../figures/artists_filtered.pdf}
  \caption{artists}
  \label{fig:filtering}
\end{figure}

\section{Data Analysis}

\subsection{Explorative Data Analysis}

\subsection{Dimensionality Reduction}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{../figures/tsne_genres.pdf}
  \caption{tsne}
  \label{fig:tsne_genres}
\end{figure}

\begin{itemize}
  \item Some homogeneous clusters arise, such as rap, classical music and metal. 
  \item Cluster neighborhoods are interesting. For instance, jazz is neighbor of classical.
  \item Also, a lot of songs are really mixed.
\end{itemize}

\subsection{Statistical Analysis}
For doing ... we applied logistic regression

\textbf{Logisitc Regression}
Logistic Regression in general enables predicting a binary variable. 
For this project, this variable is \textit{popular} vs. \textit{unpopular} for individual tracks.
For the prediction we consider all of the tracks \textit{audio features}. 
Those features are purely technical and, therefore, we inductively assume that the probability whether a song is popular is only dependent on its audio features. 

To create the labels, we threshold the popularity measure. 
We perform two different experiments. 
One tight threshold, labeling only the top \SI{10}{\percent} songs as popular, and one more general one where we use the median to divide the track into two groups.
To train the classifier we divide the dataset into \SI{80}{\percent} training and \SI{20}{\percent} test data.
Still, despite our outlier removal, the dataset contains much more examples of unpopular songs than popular songs under both thresholding criteria.
This is problematic as the model gets stuck in predicting \textit{unpopular} for all training examples. 
Many works have been published to get around this issue. We decided to use a \textit{weighted loss} function to peanalize this behavoir, which is a commonly used technique. % \cite{haixiangLearningClassimbalancedData2017a}.

For the logistic regression we apply l2-regularization and to select the best hyperparameters for the model, we use grid search and 5-fold cross validation. Then, the final models are evaluated on the test set to obtain the metrics.
In Figure~\ref{fig:roc_curve} we show the  receiver operating characteristic (ROC) for the models.
Shortly, the metric describes how the models predictions behave when varying the decision threshold. 
We observe that the shape of the curve is very smooth, corresponding to the nature of our labels.
To evaluate the quality of our models we use the area under the ROC curve.
With respect to this metric it seems like the tight model performs better at discriminating our target variable.

However, if we look closer on how the predictions are distributed (see the calibration plots in Fig.~\ref{fig:calibration}), we see that the predicted distribution of the tight classifier differs from our more general model.
For the more tight model, we have way less training examples for the popular group, than with the more general model. This could be a potential reason for the shifted distribution.

Since we aim to predict popularity in general, and not to be a real chart striker, we decided to accept the more general model.
To find out how each features influence the prediction we look into the learned coefficients of our regression model, c.f.~Fig.\ref{fig:params}.
Weights in logistic regression have a linear relationship with the \textit{odds} of an outcome of the target variable.
Our analysis shows, that especially \textit{loudness} and \textit{danceability} correspond to a higher odds of beeing popular.
These findings align with phenomena such as the \textit{loudness war}\footnote{reference} of recorded music which emerged in the last years.
Finding that \textit{danceability} correlates with higher odds of beeing popular could be pontentially be an indication that gives hints to the answer of our overall thesis - the humankind seems to like music that can be danced on.
On the flip side we also have negative correlations. 
Acousticness and instrumentalness negatively predict the odds, as well as valence and energy (which are correlated by 0.42 and have a similar interpretation).

\begin{itemize}
  \item Using multiclass logistic regression suffers from the issue that the top classes are highly imbalanced in contrast to the unpopular classes. Using weight based approaches for the loss function leads bad performances for the classes of the highly rare events. 
\end{itemize}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{../figures/roc_logistic.pdf}
  \caption{ROC curve}
  \label{fig:roc_curve}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{../figures/calibration_combined.pdf}
  \caption{Calibration}
  \label{fig:calibration}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{../figures/logistic_coefs_50_threshold_model.pdf}
  \caption{Parameters}
  \label{fig:params}
\end{figure}
  
\end{figure}

\section{Discussion}
Our work is limited because\dots
In regards to ... we cannot estimate confidence because ...

\begin{itemize}
  \item Song features lack important factors that correlate with song popularity, e.g., is the name of the artist relevant in pop culture. For the analysis we assumed that song popularity is discriminated by features that are only related on the "physical properties" of the song. However, for predicting popularity we might have to respect some "marketing aspects" as well. 
\end{itemize}


% not working, why?
% \bibliography{bibliography}

\end{document}
